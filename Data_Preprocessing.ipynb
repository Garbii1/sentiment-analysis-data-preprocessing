{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1fDyvGn1/g7fr2ycYonb2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*   **Title:** `Data Preprocessing for Sentiment Analysis`\n",
        "    *   **Objective:** The objective of this notebook is to load a raw sentiment dataset, clean it, handle missing values, encode categorical data, and prepare it for machine learning."
      ],
      "metadata": {
        "id": "Mw7S7_RcFGsl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_uMqaWhLWZ6",
        "outputId": "c11591e5-be32-41d2-eafd-7f18daa8da38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Head:\n",
            "   Unnamed: 0.1  Unnamed: 0  \\\n",
            "0             0           0   \n",
            "1             1           1   \n",
            "2             2           2   \n",
            "3             3           3   \n",
            "4             4           4   \n",
            "\n",
            "                                                Text    Sentiment  \\\n",
            "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
            "1   Traffic was terrible this morning.           ...   Negative     \n",
            "2   Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
            "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
            "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
            "\n",
            "             Timestamp            User     Platform  \\\n",
            "0  2023-01-15 12:30:00   User123          Twitter     \n",
            "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
            "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
            "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
            "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
            "\n",
            "                                     Hashtags  Retweets  Likes       Country  \\\n",
            "0   #Nature #Park                                  15.0   30.0     USA         \n",
            "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
            "2   #Fitness #Workout                              20.0   40.0   USA           \n",
            "3   #Travel #Adventure                              8.0   15.0     UK          \n",
            "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
            "\n",
            "   Year  Month  Day  Hour  \n",
            "0  2023      1   15    12  \n",
            "1  2023      1   15     8  \n",
            "2  2023      1   15    15  \n",
            "3  2023      1   15    18  \n",
            "4  2023      1   15    19  \n",
            "\n",
            "Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 732 entries, 0 to 731\n",
            "Data columns (total 15 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Unnamed: 0.1  732 non-null    int64  \n",
            " 1   Unnamed: 0    732 non-null    int64  \n",
            " 2   Text          732 non-null    object \n",
            " 3   Sentiment     732 non-null    object \n",
            " 4   Timestamp     732 non-null    object \n",
            " 5   User          732 non-null    object \n",
            " 6   Platform      732 non-null    object \n",
            " 7   Hashtags      732 non-null    object \n",
            " 8   Retweets      732 non-null    float64\n",
            " 9   Likes         732 non-null    float64\n",
            " 10  Country       732 non-null    object \n",
            " 11  Year          732 non-null    int64  \n",
            " 12  Month         732 non-null    int64  \n",
            " 13  Day           732 non-null    int64  \n",
            " 14  Hour          732 non-null    int64  \n",
            "dtypes: float64(2), int64(6), object(7)\n",
            "memory usage: 85.9+ KB\n"
          ]
        }
      ],
      "source": [
        "# Import the pandas library for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from the uploaded file\n",
        "df = pd.read_csv('3) Sentiment dataset.csv')\n",
        "\n",
        "# Display the first 5 rows to see what the data looks like\n",
        "print(\"Original Data Head:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display a summary of the dataframe to see data types and non-null counts\n",
        "print(\"\\nData Info:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Headline:** `Handling Missing Data`\n",
        "    *   **Explanation:** First, I checked for missing values. The 'Retweets' and 'Likes' columns had missing data, which I filled with the median value to avoid skewing the data with outliers. Rows with missing 'Country' were dropped."
      ],
      "metadata": {
        "id": "I74j3S_zGDD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for the total number of missing values in each column\n",
        "print(\"Missing values before handling:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# For numerical columns like 'Retweets' and 'Likes', we'll fill missing values with the median.\n",
        "# The median is less sensitive to outliers than the mean.\n",
        "median_retweets = df['Retweets'].median()\n",
        "df['Retweets'] = df['Retweets'].fillna(median_retweets)\n",
        "\n",
        "median_likes = df['Likes'].median()\n",
        "df['Likes'] = df['Likes'].fillna(median_likes)\n",
        "\n",
        "# For the 'Hashtags' and 'Country' columns, there are only a few missing values.\n",
        "# For simplicity in this task, we will drop the rows where these are missing.\n",
        "df.dropna(subset=['Hashtags', 'Country'], inplace=True)\n",
        "\n",
        "# Verify that the missing values have been handled\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TD4Z0GjNT5y",
        "outputId": "6b5c0136-bb49-42da-9bd6-5f48feebd333"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values before handling:\n",
            "Unnamed: 0.1    0\n",
            "Unnamed: 0      0\n",
            "Text            0\n",
            "Sentiment       0\n",
            "Timestamp       0\n",
            "User            0\n",
            "Platform        0\n",
            "Hashtags        0\n",
            "Retweets        0\n",
            "Likes           0\n",
            "Country         0\n",
            "Year            0\n",
            "Month           0\n",
            "Day             0\n",
            "Hour            0\n",
            "dtype: int64\n",
            "\n",
            "Missing values after handling:\n",
            "Unnamed: 0.1    0\n",
            "Unnamed: 0      0\n",
            "Text            0\n",
            "Sentiment       0\n",
            "Timestamp       0\n",
            "User            0\n",
            "Platform        0\n",
            "Hashtags        0\n",
            "Retweets        0\n",
            "Likes           0\n",
            "Country         0\n",
            "Year            0\n",
            "Month           0\n",
            "Day             0\n",
            "Hour            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Headline:** `Encoding Categorical Data`\n",
        "    *   **Explanation:** Machine learning models require numerical input, so I converted the text-based 'Platform' and 'Sentiment' columns into numerical format using one-hot encoding with pandas' `get_dummies` function."
      ],
      "metadata": {
        "id": "XmMOxVzZGT0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the categorical columns we want to encode.\n",
        "# We'll choose 'Platform' and 'Sentiment' as examples.\n",
        "categorical_cols = ['Platform', 'Sentiment']\n",
        "\n",
        "# Use pandas get_dummies() to perform one-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Display the first few rows to see the new columns\n",
        "# You will see new columns like 'Platform_Twitter', 'Sentiment_Positive', etc.\n",
        "print(\"Data after One-Hot Encoding:\")\n",
        "print(df_encoded.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTmQuks2SZMj",
        "outputId": "7bfe85fd-2a1a-462a-b9fa-3e1892aa5113"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after One-Hot Encoding:\n",
            "   Unnamed: 0.1  Unnamed: 0  \\\n",
            "0             0           0   \n",
            "1             1           1   \n",
            "2             2           2   \n",
            "3             3           3   \n",
            "4             4           4   \n",
            "\n",
            "                                                Text            Timestamp  \\\n",
            "0   Enjoying a beautiful day at the park!        ...  2023-01-15 12:30:00   \n",
            "1   Traffic was terrible this morning.           ...  2023-01-15 08:45:00   \n",
            "2   Just finished an amazing workout! ðŸ’ª          ...  2023-01-15 15:45:00   \n",
            "3   Excited about the upcoming weekend getaway!  ...  2023-01-15 18:20:00   \n",
            "4   Trying out a new recipe for dinner tonight.  ...  2023-01-15 19:55:00   \n",
            "\n",
            "             User                                    Hashtags  Retweets  \\\n",
            "0   User123         #Nature #Park                                  15.0   \n",
            "1   CommuterX       #Traffic #Morning                               5.0   \n",
            "2   FitnessFan      #Fitness #Workout                              20.0   \n",
            "3   AdventureX      #Travel #Adventure                              8.0   \n",
            "4   ChefCook        #Cooking #Food                                 12.0   \n",
            "\n",
            "   Likes       Country  Year  ...  Sentiment_ Vibrancy   \\\n",
            "0   30.0     USA        2023  ...                 False   \n",
            "1   10.0     Canada     2023  ...                 False   \n",
            "2   40.0   USA          2023  ...                 False   \n",
            "3   15.0     UK         2023  ...                 False   \n",
            "4   25.0    Australia   2023  ...                 False   \n",
            "\n",
            "   Sentiment_ Whimsy          Sentiment_ Whispers of the Past   \\\n",
            "0                      False                             False   \n",
            "1                      False                             False   \n",
            "2                      False                             False   \n",
            "3                      False                             False   \n",
            "4                      False                             False   \n",
            "\n",
            "   Sentiment_ Winter Magic   Sentiment_ Wonder   Sentiment_ Wonder       \\\n",
            "0                     False               False                   False   \n",
            "1                     False               False                   False   \n",
            "2                     False               False                   False   \n",
            "3                     False               False                   False   \n",
            "4                     False               False                   False   \n",
            "\n",
            "   Sentiment_ Wonder         Sentiment_ Wonderment      Sentiment_ Yearning   \\\n",
            "0                     False                      False                 False   \n",
            "1                     False                      False                 False   \n",
            "2                     False                      False                 False   \n",
            "3                     False                      False                 False   \n",
            "4                     False                      False                 False   \n",
            "\n",
            "   Sentiment_ Zest   \n",
            "0             False  \n",
            "1             False  \n",
            "2             False  \n",
            "3             False  \n",
            "4             False  \n",
            "\n",
            "[5 rows x 294 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Headline:** `Scaling Numerical Features`\n",
        "    *   **Explanation:** Distance-based algorithms like K-Nearest Neighbors are sensitive to the scale of the data. Features with large value ranges (like 'Likes') could disproportionately influence the model's predictions. To prevent this, I used scikit-learn's StandardScaler to transform the numerical features so they all have a mean of 0 and a standard deviation of 1, ensuring all features are treated equally."
      ],
      "metadata": {
        "id": "HnYzIWKOHPSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Select the numerical columns to scale\n",
        "numerical_features = ['Retweets', 'Likes', 'Year', 'Month', 'Day', 'Hour']\n",
        "\n",
        "# Apply the scaler to our numerical features\n",
        "df_encoded[numerical_features] = scaler.fit_transform(df_encoded[numerical_features])\n",
        "\n",
        "print(\"Data after scaling numerical features:\")\n",
        "print(df_encoded.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqmqhwFCmzX4",
        "outputId": "60079fcd-e6c5-48b4-fe77-8f5e4881abb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after scaling numerical features:\n",
            "   Unnamed: 0.1  Unnamed: 0  \\\n",
            "0             0           0   \n",
            "1             1           1   \n",
            "2             2           2   \n",
            "3             3           3   \n",
            "4             4           4   \n",
            "\n",
            "                                                Text            Timestamp  \\\n",
            "0   Enjoying a beautiful day at the park!        ...  2023-01-15 12:30:00   \n",
            "1   Traffic was terrible this morning.           ...  2023-01-15 08:45:00   \n",
            "2   Just finished an amazing workout! ðŸ’ª          ...  2023-01-15 15:45:00   \n",
            "3   Excited about the upcoming weekend getaway!  ...  2023-01-15 18:20:00   \n",
            "4   Trying out a new recipe for dinner tonight.  ...  2023-01-15 19:55:00   \n",
            "\n",
            "             User                                    Hashtags  Retweets  \\\n",
            "0   User123         #Nature #Park                             -0.922303   \n",
            "1   CommuterX       #Traffic #Morning                         -2.339444   \n",
            "2   FitnessFan      #Fitness #Workout                         -0.213733   \n",
            "3   AdventureX      #Travel #Adventure                        -1.914302   \n",
            "4   ChefCook        #Cooking #Food                            -1.347445   \n",
            "\n",
            "      Likes       Country      Year  ...  Sentiment_ Vibrancy   \\\n",
            "0 -0.916295     USA        0.902984  ...                 False   \n",
            "1 -2.336727     Canada     0.902984  ...                 False   \n",
            "2 -0.206079   USA          0.902984  ...                 False   \n",
            "3 -1.981619     UK         0.902984  ...                 False   \n",
            "4 -1.271403    Australia   0.902984  ...                 False   \n",
            "\n",
            "   Sentiment_ Whimsy          Sentiment_ Whispers of the Past   \\\n",
            "0                      False                             False   \n",
            "1                      False                             False   \n",
            "2                      False                             False   \n",
            "3                      False                             False   \n",
            "4                      False                             False   \n",
            "\n",
            "   Sentiment_ Winter Magic   Sentiment_ Wonder   Sentiment_ Wonder       \\\n",
            "0                     False               False                   False   \n",
            "1                     False               False                   False   \n",
            "2                     False               False                   False   \n",
            "3                     False               False                   False   \n",
            "4                     False               False                   False   \n",
            "\n",
            "   Sentiment_ Wonder         Sentiment_ Wonderment      Sentiment_ Yearning   \\\n",
            "0                     False                      False                 False   \n",
            "1                     False                      False                 False   \n",
            "2                     False                      False                 False   \n",
            "3                     False                      False                 False   \n",
            "4                     False                      False                 False   \n",
            "\n",
            "   Sentiment_ Zest   \n",
            "0             False  \n",
            "1             False  \n",
            "2             False  \n",
            "3             False  \n",
            "4             False  \n",
            "\n",
            "[5 rows x 294 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Headline:** `Splitting the Dataset into Training and Testing Sets`\n",
        "    *   **Explanation:** The final step of preprocessing is to split the data. I allocated 80% of the data for training the model (X_train, y_train) and reserved the remaining 20% as a 'final exam' for testing (X_test, y_test). This ensures that we can evaluate the model's performance on unseen data to get an unbiased measure of its effectiveness.\n",
        "  "
      ],
      "metadata": {
        "id": "ZlBjegRBJPj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First, we need to define our features (X) and our target (y).\n",
        "# Let's say our goal is to predict if a sentiment is positive.\n",
        "# We need to make sure the target variable exists from the encoding step.\n",
        "# Our encoded columns include 'Platform_Twitter', 'Sentiment_Positive', etc.\n",
        "# We will drop columns that are not useful for prediction (like text, user IDs).\n",
        "\n",
        "# Clean up column names by removing leading and trailing spaces from sentiment columns\n",
        "df_encoded.columns = [col.strip() if col.startswith('Sentiment_') else col for col in df_encoded.columns]\n",
        "\n",
        "# Print column names for debugging after stripping\n",
        "print(\"Columns in df_encoded after stripping:\")\n",
        "print(df_encoded.columns)\n",
        "\n",
        "# Find the exact column name for 'Positive' sentiment after encoding and stripping\n",
        "positive_sentiment_col = None\n",
        "for col in df_encoded.columns:\n",
        "    if col.startswith('Sentiment_') and 'Positive' in col:\n",
        "        positive_sentiment_col = col\n",
        "        break\n",
        "\n",
        "if positive_sentiment_col is None:\n",
        "    raise ValueError(\"Could not find the 'Positive' sentiment column after encoding and stripping.\")\n",
        "\n",
        "# Drop irrelevant columns and all sentiment columns except the target variable\n",
        "columns_to_drop = [col for col in df_encoded.columns if col.startswith('Sentiment_') and col != positive_sentiment_col] + ['Unnamed: 0', 'Text', 'Timestamp', 'User', 'Hashtags', 'Country', 'Unnamed: 0.1']\n",
        "X = df_encoded.drop(columns=columns_to_drop)\n",
        "y = df_encoded[positive_sentiment_col] # Target: predicting positive sentiment\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the new datasets to confirm the split\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSmM3IAItkhl",
        "outputId": "1677287b-a2ec-4b99-9b73-6bed3c33065f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in df_encoded after stripping:\n",
            "Index(['Unnamed: 0.1', 'Unnamed: 0', 'Text', 'Timestamp', 'User', 'Hashtags',\n",
            "       'Retweets', 'Likes', 'Country', 'Year',\n",
            "       ...\n",
            "       'Sentiment_ Vibrancy', 'Sentiment_ Whimsy',\n",
            "       'Sentiment_ Whispers of the Past', 'Sentiment_ Winter Magic',\n",
            "       'Sentiment_ Wonder', 'Sentiment_ Wonder', 'Sentiment_ Wonder',\n",
            "       'Sentiment_ Wonderment', 'Sentiment_ Yearning', 'Sentiment_ Zest'],\n",
            "      dtype='object', length=294)\n",
            "Shape of X_train: (585, 11)\n",
            "Shape of X_test: (147, 11)\n",
            "Shape of y_train: (585, 2)\n",
            "Shape of y_test: (147, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PAU51BLym92"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}